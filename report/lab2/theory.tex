\subsection{Схема решения задачи интервальной регрессии}
Будем, как и в прошлой работе, отдельно решать задачу интервальной регресии для двух наборов входных данных $(I, \mathbf{Y_1}), (I, \mathbf{Y_2})$. Здесь $I$ -- номера измерений, $\mathbf{Y_1}, \mathbf{Y_2}$ -- обынтерваленные измеренные значения. В отличие от первой работы, будем решать эти задачи как задачи интервальной, а не вещественной регрессии, описанным ниже способом.

Далее, аналогично предыдущей работе, найдём оптимальный коэффициент $R_{21}$, максимизируя коэффициент Жаккара.

\subsection{Интервальная регрессия как задача оптимизации}
В данной работе для решения задачи интервальной регрессии будем использовать следующий подход.

Будем искать зависимость $y^{(k)} = \beta_0^{(k)} + \beta_1^{(k)}x$ таким образом, чтобы, минимально расширив интервалы исходного интервального вектора $\{\mathbf{y_i}\}_{i=1}^{n}$, получить набор интервалов, накрывающий аппроксимирующую прямую:

\begin{equation} \label{regropt}
\begin{cases}
\mathtt{mid} \mathbf{y^{(k)}_i} - w^{(k)}_i \cdot \mathtt{rad} \mathbf{y^{(k)}_i} \leq \beta_0^{(k)} + \beta_1^{(k)} i \leq \mathtt{mid} \mathbf{y^{(k)}_i} + w^{(k)}_i \cdot \mathtt{rad} \mathbf{y^{(k)}_i} & , \; i = \overline{1, n}\\
\displaystyle\sum_{i=1}^{n} w_i^{(k)} \longrightarrow \min & \\
w_i^{(k)} \geq 0 & , \; i = \overline{1, n} \\
w^{(k)}, \beta_0^{(k)}, \beta_1^{(k)} = ? &
\end{cases}
\end{equation}

Здесь $k \in \{1, 2\}$ -- номер набора данных.

Данная задача является задачей линейного программирования. Как и в прошлой работе, примем $\varepsilon := \mathtt{rad} \mathbf{y_i^{(k)}} = 10^{-4}$ для всех $i = \overline{1, n}$.

\subsection{Информационное множество}

Применительно к данной задаче, информационное множество -- это все такие пары $(\beta_0, \beta_1)$, при которых выполнено первое ограничение типа неравенства задачи оптимизации \ref{regropt}.


\subsection{Коридор совместных зависимостей}

В постановке задаче оптимизации \ref{regropt} не ставится никаких ограничений и целей по минимизации для параметров $\beta_0, \beta_1$. Ясно, что параметры $\beta_0, \beta_1$, полученные в результате решения задачи оптимизации, будут не единственными допустимыми: информационное множество задает целое семейство допустимых $\beta_0, \beta_1$. Следовательно, имеет смысл рассматривать, как единое целое, множество всех функций, совместных с интервальными данными задачи восстановления зависимостей. Такое множество называется \textbf{коридором совместности}.
\textbf{Граничными} называются измерения, определяющие какой-либо фрагмент границы множества. Это свойство имеет смысл рассматривать для наблюдений, принадлежащих выборке, по которой строилась модель. Граничные измерения задают минимальную подвыборку, определяющую модель.